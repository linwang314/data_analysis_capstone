---
title: "Cousera Capstone Project Milestone Report"
author: "Lin Wang"
date: "March 19, 2016"
output: html_document
---
## Summary  

The objective of this capstone project is to build a predictive text model that can enhance typing experience. A large English database consisting of text from blogs, news and twitter is used for this purpose. This report provides a summary of the transformation and exploratory analysis of the data.  A random text sample from the original database is selected for this preliminary investigation and model buildup. The frequencies of single word, 2-grams and 3-grams provided by this report will facilitate understanding of the language data and model development. For more details on the scripts used for this report, see the [Github page](http://github.com/linwang929/data_analysis_capstone).

## Introduction  

Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. SwiftKey, our corporate partner in this capstone, builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. In this project, the skills acquired in Data Science Specialization are applied to a new field - text mining and natural language processing. Starting from the basics of cleaning and exploratory analysis, we will understand the structure and characteristics of the language data, and ultimately build a predictive text model.  

## Data  

The data is from a corpus called [HC Corpora](www.corpora.heliohost.org). See the readme file at <http://www.corpora.heliohost.org/aboutcorpus.html> for details on the corpora available. The files have been language filtered but may still contain some foreign text. 

### Loading Data

Here we use R's readLines function with file connections. 

```{r, eval=FALSE}
## create connections
con1 <- file("C:/Users/linwa_000/Desktop/final/en_US/en_US.blogs.txt", "rb")
con2 <- file("C:/Users/linwa_000/Desktop/final/en_US/en_US.news.txt", "rb")
con3 <- file("C:/Users/linwa_000/Desktop/final/en_US/en_US.twitter.txt", "rb")

## read files
blog <- readLines(con1)
news <- readLines(con2)
twitter <- readLines(con3)

## close connections
close(con1, con2, con3)
rm(con1, con2, con3)```
```

### Basic Assessment 

```{r, eval=FALSE}
## number of lines
length(blog)  ## 899,288
length(news)  ## 1,010,242
length(twitter)  ## 2,360,148

## number of words
sum(sapply(strsplit(blog, split = "[^A-Za-z0-9]"), length))  ## 44,729,620
sum(sapply(strsplit(news, split = "[^A-Za-z0-9]"), length))  ## 41,076,735
sum(sapply(strsplit(twitter, split = "[^A-Za-z0-9]"), length))  ## 37,108,123
```

The three databases (blogs, news, and twitter) have 899288, 1010242, 2360148 lines, and 44729620, 41076735, 37108123 words respectively. 

### Sampling Strategy

This is a fairly large dataset. Often relatively few randomly selected rows or chunks are enough to obtain an accurate approximation for exploratory analysis and for model development. The rbinom function is used here to "flip a biased coin" to determine whether to sample a line of text or not, and 1% of the original text is selected in the sample. A separate sub-sample dataset is created and written out to a separate file for future uses. 

```{r, eval=FALSE}
set.seed(1122)
sample.index1 <- as.logical(rbinom(n = length(blog), size = 1, prob = 0.01))
sample.index2 <- as.logical(rbinom(n = length(news), size = 1, prob = 0.01))
sample.index3 <- as.logical(rbinom(n = length(twitter), size = 1, prob = 0.01))

sampletext <- c(blog[sample.index1], news[sample.index2], twitter[sample.index3])
writeLines(sampletext, con = "C:/Users/linwa_000/Dropbox/Coursera_Data Science Specialization/Capstone Project/sample.txt")
```

## Data Transformation and Processing

### Tokenization and Profanity filtering

Tokenization is the process to identifying appropriate tokens such as words, punctuation, and numbers. Filtering can remove profanity and other words that we don't want to predict. Tools in R pacakges NLP and tm are used to generate the two functions below: transtext() function takes character vectors as input, removes punctuation, stopwords, numbers and words we don't want, and returns a corpus of documents; mytokenizer() function takes a corpus of documents as input, and change them into a tokenized character vector. 
```{r, eval=FALSE}
require(NLP)
library(tm)
library(SnowballC)
## function for data transformation
transtext <- function(text, filterword) {
  ## build a corpus, which is a collection of text documents
  myCorpus <- Corpus(VectorSource(text))
  ## transfer to lower cases
  myCorpus <- tm_map(myCorpus, tolower)
  ## remove stopwords 
  myCorpus <- tm_map(myCorpus, removeWords, stopwords("en"))
  ## remove profanity and words we don't want
  myCorpus <- tm_map(myCorpus, removeWords, filterword)
  ## remove punctuation
  myCorpus <- tm_map(myCorpus, removePunctuation)
  ## remove numbers
  myCorpus <- tm_map(myCorpus, removeNumbers)
  ## stemming
  myCorpus <- tm_map(myCorpus, stemDocument)
  
  ## the next step is to correct the data type. See details at:
  ## http://stackoverflow.com/questions/24771165/r-project-no-applicable-method-for-meta-applied-to-an-object-of-class-charact
  myCorpus <- tm_map(myCorpus, PlainTextDocument)
  return(myCorpus)
}

## function to transfer corpus back to character vector
mytokenizer <- function(x) {
  y <- sapply(x[1:length(x)], as.character)
  z <- scan_tokenizer(y)
  return(z)
}
```

We can use the above funtions to find the tokenized version of our sample text. The profanity word list comes from [this website](http://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/)
```{r, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
sampletext <- readLines("C:/Users/linwa_000/Dropbox/Coursera_Data Science Specialization/Capstone Project/sample.txt")
source("C:/Users/linwa_000/Dropbox/Coursera_Data Science Specialization/Capstone Project/tokenization.R")
```
```{r, warning=FALSE, cache=TRUE}
profanity <- readLines("C:/Users/linwa_000/Dropbox/Coursera_Data Science Specialization/Capstone Project/badwords.txt")
sample1 <- mytokenizer(transtext(sampletext, profanity))
```

## Exploratory Data Analysis

The main objective of this section is to analyze the frequencies of words (1-grams), word pairs (2-grams) and 3-grams and to understand the distribution and relationship of word and phrases. 

#### Finding n-grams from tokenized text  

ngrams() function from NLP package is used to find 2-grams and 3-grams from the tokenized version in the last step. 

```{r, cache=TRUE}
sample2 <- vapply(ngrams(sample1, 2), paste, "", collapse = " ")
sample3 <- vapply(ngrams(sample1, 3), paste, "", collapse = " ")
```

#### Count and record frequency

```{r, cache=TRUE}
sample1df <- data.frame(table(sample1))
sample2df <- data.frame(table(sample2))
sample3df <- data.frame(table(sample3))
```

#### Sort based on frequency
```{r, cache=TRUE}
library(dplyr)
onegram <- arrange(sample1df, desc(Freq))
twogram <- arrange(sample2df, desc(Freq))
thrgram <- arrange(sample3df, desc(Freq))

names(onegram) <- c("word", "frequency")
names(twogram) <- c("phrase", "frequency")
names(thrgram) <- c("phrase", "frequency")
```

#### Plotting the most frequent words/phrases  

```{r, cache=TRUE}
library(ggplot2)
## word
ggplot(data = onegram[1:40, ], aes(x = reorder(word, order(frequency, decreasing = TRUE)), y = frequency)) + 
  geom_bar(stat = "identity") + labs(x = "word") + labs(title = "Word Frequency") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
## 2-grams
ggplot(data = twogram[1:40, ], aes(x = reorder(phrase, order(frequency, decreasing = TRUE)), y = frequency)) + 
  geom_bar(stat = "identity") + labs(x = "phrase") + labs(title = "2-gram Frequency") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
## 3-grams
ggplot(data = thrgram[1:40, ], aes(x = reorder(phrase, order(frequency, decreasing = TRUE)), y = frequency)) + 
  geom_bar(stat = "identity") + labs(x = "phrase") + labs(title = "3-gram Frequency") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### Coverage

The following function takes a character vector and a percentage coverage as input and return the number of words needed to cover a certain percentage of all word instances. 

```{r, eval=FALSE}
cover <- function(token, coverage) {
  freq <- data.frame(table(token))
  names(freq) <- c("phrase", "frequency")
  freq <- arrange(freq, desc(frequency))
  m <- length(token)*coverage
  j <- 1
  counts <- 0
  while (counts < m) {
    counts <- counts + freq$frequency[j]
    j <- j+1
  }
  return(j-1)
}
```

```{r, eval=FALSE}
## number of words to cover 50%
cover(sample1, 0.5)
## number of words to cover 90%
cover(sample1, 0.9)
```

We can find out that to achieve 50% and 90% coverages, we need 1064 and 17110 words respectively. 

## Conclusions and Future Work  

In this section we have a basic understanding of the structure and characteristics of the language database and find out the distribution of words and phrases. Based on these results, the next tasks and steps would be:  

* enhance word coverage  
* build a basic n-gram model  
* optimize algorithm for big data processing  
